#!/usr/bin/env python3
"""
EDA Runner - æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æã®ãŸã‚ã®CLIã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ³•:
    python tools/eda_runner.py
    python tools/eda_runner.py --train-path data/train.csv --text-col comment_text --label-col target
    EDA_TRAIN_PATH=data/train.csv python tools/eda_runner.py --out-dir output/eda
"""

import argparse
import os
import json
import warnings
import glob
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Import shared helper
from utils.find_train_csv import find_train_csv

warnings.filterwarnings('ignore')

# matplotlibè¨­å®šï¼ˆéGUIãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼‰
plt.switch_backend('Agg')
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10


def detect_text_column(df):
    """ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’è‡ªå‹•æ¤œå‡º"""
    candidates = ['text', 'comment_text', 'content', 'document', 'text_clean', 'body']
    for col in candidates:
        if col in df.columns:
            return col
    return None


def detect_label_column(df):
    """ãƒ©ãƒ™ãƒ«åˆ—ã‚’è‡ªå‹•æ¤œå‡º"""
    candidates = ['target', 'label', 'toxic', 'y', 'rule_violation']
    for col in candidates:
        if col in df.columns:
            return col
    return None


def load_data_with_encoding(file_path):
    """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
    
    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            print(f"âœ“ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° {encoding} ã§èª­ã¿è¾¼ã¿æˆåŠŸ")
            return df
        except UnicodeDecodeError:
            continue
    
    raise ValueError("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


def analyze_text_length(df, text_col, output_dir):
    """ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ†æ"""
    print("ğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ†æã‚’å®Ÿè¡Œä¸­...")
    
    # ãƒ†ã‚­ã‚¹ãƒˆé•·ç‰¹å¾´é‡è¨ˆç®—
    text_data = df[text_col].fillna("").astype(str)
    df['char_len'] = text_data.str.len()
    df['token_len'] = text_data.str.split().str.len()
    
    # çµ±è¨ˆæƒ…å ±
    char_stats = df['char_len'].describe().to_dict()
    token_stats = df['token_len'].describe().to_dict()
    
    print(f"  æ–‡å­—é•·çµ±è¨ˆ: å¹³å‡ {char_stats['mean']:.1f}, æœ€å¤§ {char_stats['max']}")
    print(f"  ãƒˆãƒ¼ã‚¯ãƒ³é•·çµ±è¨ˆ: å¹³å‡ {token_stats['mean']:.1f}, æœ€å¤§ {token_stats['max']}")
    
    # æ–‡å­—é•·ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    plt.figure(figsize=(12, 6))
    plt.hist(df['char_len'], bins=50, alpha=0.7, edgecolor='black')
    plt.title('ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—é•·åˆ†å¸ƒ')
    plt.xlabel('æ–‡å­—æ•°')
    plt.ylabel('é »åº¦')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/text_length_chars.png", dpi=100, bbox_inches='tight')
    plt.close()
    
    # ãƒˆãƒ¼ã‚¯ãƒ³é•·ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    plt.figure(figsize=(12, 6))
    plt.hist(df['token_len'], bins=50, alpha=0.7, edgecolor='black')
    plt.title('ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³é•·åˆ†å¸ƒ')
    plt.xlabel('ãƒˆãƒ¼ã‚¯ãƒ³æ•°')
    plt.ylabel('é »åº¦')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/text_length_tokens.png", dpi=100, bbox_inches='tight')
    plt.close()
    
    return {
        "char_len": char_stats,
        "token_len": token_stats
    }


def analyze_label_distribution(df, label_col, output_dir):
    """ãƒ©ãƒ™ãƒ«åˆ†å¸ƒåˆ†æ"""
    print("ğŸ¯ ãƒ©ãƒ™ãƒ«åˆ†å¸ƒåˆ†æã‚’å®Ÿè¡Œä¸­...")
    
    label_data = df[label_col].dropna()
    
    # æ•°å€¤å‹ã‹ã‚«ãƒ†ã‚´ãƒªå‹ã‹ã‚’åˆ¤å®š
    if pd.api.types.is_numeric_dtype(label_data):
        print("  ãƒ©ãƒ™ãƒ«ã‚¿ã‚¤ãƒ—: æ•°å€¤å‹")
        stats = label_data.describe().to_dict()
        
        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        plt.figure(figsize=(12, 6))
        plt.hist(label_data, bins=30, alpha=0.7, edgecolor='black')
        plt.title('ãƒ©ãƒ™ãƒ«åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰')
        plt.xlabel('ãƒ©ãƒ™ãƒ«å€¤')
        plt.ylabel('é »åº¦')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/label_hist.png", dpi=100, bbox_inches='tight')
        plt.close()
        
        return {
            "type": "numeric",
            "stats": stats
        }
        
    else:
        print("  ãƒ©ãƒ™ãƒ«ã‚¿ã‚¤ãƒ—: ã‚«ãƒ†ã‚´ãƒªå‹")
        value_counts = label_data.value_counts()
        print(f"  ã‚¯ãƒ©ã‚¹æ•°: {len(value_counts)}")
        
        # æ£’ã‚°ãƒ©ãƒ•
        plt.figure(figsize=(12, 6))
        value_counts.plot(kind='bar', alpha=0.7)
        plt.title('ãƒ©ãƒ™ãƒ«åˆ†å¸ƒï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰')
        plt.xlabel('ãƒ©ãƒ™ãƒ«')
        plt.ylabel('é »åº¦')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/label_bar.png", dpi=100, bbox_inches='tight')
        plt.close()
        
        return {
            "type": "categorical",
            "value_counts": value_counts.to_dict()
        }


def analyze_duplicates(df, text_col, output_dir):
    """é‡è¤‡æ¤œå‡ºåˆ†æ"""
    print("ğŸ” é‡è¤‡æ¤œå‡ºåˆ†æã‚’å®Ÿè¡Œä¸­...")
    
    # ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–
    normalized_text = df[text_col].fillna("").astype(str).str.lower().str.strip()
    normalized_text = normalized_text.str.replace(r'\s+', ' ', regex=True)
    
    # é‡è¤‡ã‚«ã‚¦ãƒ³ãƒˆ
    duplicate_counts = normalized_text.value_counts()
    duplicates = duplicate_counts[duplicate_counts > 1]
    
    duplicate_total = duplicates.sum() - len(duplicates)
    print(f"  é‡è¤‡ã‚µãƒ³ãƒ—ãƒ«æ•°: {duplicate_total}")
    print(f"  é‡è¤‡ãƒ†ã‚­ã‚¹ãƒˆç¨®é¡æ•°: {len(duplicates)}")
    
    if len(duplicates) > 0:
        # CSVä¿å­˜
        top_duplicates = duplicates.head(10)
        duplicates_df = pd.DataFrame({
            'text': top_duplicates.index,
            'count': top_duplicates.values
        })
        duplicates_df.to_csv(f"{output_dir}/duplicates_top.csv", index=False, encoding='utf-8')
        
        return {
            "duplicate_count": int(duplicate_total),
            "has_duplicates": True,
            "duplicate_types": len(duplicates)
        }
    else:
        return {
            "duplicate_count": 0,
            "has_duplicates": False,
            "duplicate_types": 0
        }


def analyze_groups(df, output_dir):
    """ã‚°ãƒ«ãƒ¼ãƒ—/ãƒªãƒ¼ã‚¯æ¤œæŸ»"""
    print("ğŸ‘¥ ã‚°ãƒ«ãƒ¼ãƒ—/ãƒªãƒ¼ã‚¯æ¤œæŸ»ã‚’å®Ÿè¡Œä¸­...")
    
    group_candidates = ['user_id', 'author', 'identity', 'comment_id', 'thread_id', 'post_id']
    found_groups = [col for col in group_candidates if col in df.columns]
    
    groups_info = {
        "found_columns": found_groups,
        "recommendations": []
    }
    
    if found_groups:
        print(f"  æ¤œå‡ºã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—åˆ—: {found_groups}")
        
        for col in found_groups:
            nunique = df[col].nunique()
            n_samples = len(df)
            top_freq = df[col].value_counts().head(5)
            
            print(f"    {col}: ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•° {nunique}")
            
            # GroupKFoldã®æ¨å¥¨åˆ¤å®š
            if nunique < n_samples * 0.5:
                groups_info["recommendations"].append(f"{col}: GroupKFoldæ¨å¥¨ (ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {nunique})")
                print(f"      â†’ GroupKFoldæ¨å¥¨")
            
            groups_info[col] = {
                "nunique": nunique,
                "top_frequencies": top_freq.to_dict()
            }
    else:
        print("  ã‚°ãƒ«ãƒ¼ãƒ—åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        groups_info["recommendations"].append("ã‚°ãƒ«ãƒ¼ãƒ—åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€é€šå¸¸ã®StratifiedKFoldã‚’ä½¿ç”¨")

    # groups.jsonä¿å­˜
    with open(f"{output_dir}/groups.json", "w", encoding="utf-8") as f:
        json.dump(groups_info, f, ensure_ascii=False, indent=2)

    return groups_info


def analyze_time_leakage(df, output_dir):
    """æ™‚ç³»åˆ—ãƒªãƒ¼ã‚¯æ¤œæŸ»"""
    print("â° æ™‚ç³»åˆ—ãƒªãƒ¼ã‚¯æ¤œæŸ»ã‚’å®Ÿè¡Œä¸­...")
    
    time_candidates = [col for col in df.columns if any(keyword in col.lower() for keyword in ['date', 'time', 'created', 'posted'])]
    
    if time_candidates:
        print(f"  æ¤œå‡ºã•ã‚ŒãŸæ™‚é–“åˆ—: {time_candidates}")
        
        time_info = {}
        
        for col in time_candidates:
            try:
                # æ—¥æ™‚å¤‰æ›ã‚’è©¦è¡Œ
                datetime_series = pd.to_datetime(df[col], errors='coerce')
                non_null_dates = datetime_series.dropna()
                
                if len(non_null_dates) > 0:
                    print(f"    {col}: æœ‰åŠ¹ãªæ—¥æ™‚ãƒ‡ãƒ¼ã‚¿ {len(non_null_dates)}ä»¶")
                    
                    # æœˆåˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
                    monthly_counts = non_null_dates.dt.to_period('M').value_counts().sort_index()
                    
                    # æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ
                    plt.figure(figsize=(12, 6))
                    monthly_counts.plot(kind='bar')
                    plt.title(f'æ™‚ç³»åˆ—åˆ†å¸ƒ - {col}')
                    plt.xlabel('æœŸé–“')
                    plt.ylabel('ãƒ‡ãƒ¼ã‚¿æ•°')
                    plt.xticks(rotation=45)
                    plt.tight_layout()
                    plt.savefig(f"{output_dir}/time_counts.png", dpi=100, bbox_inches='tight')
                    plt.close()
                    
                    time_info[col] = {
                        "valid_count": len(non_null_dates),
                        "min_date": str(non_null_dates.min()),
                        "max_date": str(non_null_dates.max()),
                        "monthly_counts": {str(k): v for k, v in monthly_counts.to_dict().items()}
                    }
                    
            except Exception as e:
                print(f"    {col}: æ—¥æ™‚å¤‰æ›ã‚¨ãƒ©ãƒ¼ - {e}")
        
        return time_info
    else:
        print("  æ™‚é–“é–¢é€£åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return {}


def analyze_correlations(df, label_col, output_dir):
    """æ•°å€¤ç›¸é–¢åˆ†æ"""
    if not pd.api.types.is_numeric_dtype(df[label_col]):
        print("âš ï¸  ãƒ©ãƒ™ãƒ«ãŒæ•°å€¤å‹ã§ã¯ãªã„ãŸã‚ã€æ•°å€¤ç›¸é–¢åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return {"message": "ãƒ©ãƒ™ãƒ«ãŒæ•°å€¤å‹ã§ã¯ãªã„"}
    
    print("ğŸ“ˆ æ•°å€¤ç›¸é–¢åˆ†æã‚’å®Ÿè¡Œä¸­...")
    
    # æ•°å€¤åˆ—ã®é¸æŠï¼ˆIDã£ã½ã„åˆ—ã¯é™¤å¤–ï¼‰
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # IDåˆ—ã®é™¤å¤–
    id_keywords = ['id', 'index', 'row']
    filtered_cols = [col for col in numeric_cols if not any(keyword in col.lower() for keyword in id_keywords)]
    
    if len(filtered_cols) > 1:  # ãƒ©ãƒ™ãƒ«åˆ—ä»¥å¤–ã«æ•°å€¤åˆ—ãŒã‚ã‚‹
        correlation_data = df[filtered_cols].corr()
        label_corrs = correlation_data[label_col].drop(label_col).sort_values(key=abs, ascending=False)
        
        print(f"  æ•°å€¤ç‰¹å¾´é‡æ•°: {len(filtered_cols)-1}")
        print(f"  æœ€é«˜ç›¸é–¢: {label_corrs.iloc[0]:.4f} ({label_corrs.index[0]})")
        
        # ç›¸é–¢CSVä¿å­˜
        label_corrs.to_csv(f"{output_dir}/correlations.csv", header=['correlation'])
        
        # ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆä¸Šä½20å€‹ï¼‰
        top_corr_cols = [label_col] + label_corrs.head(19).index.tolist()
        correlation_subset = correlation_data.loc[top_corr_cols, top_corr_cols]
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(correlation_subset, annot=True, cmap='coolwarm', center=0, fmt='.3f')
        plt.title('æ•°å€¤ç‰¹å¾´é‡ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆä¸Šä½20ï¼‰')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/correlation_heatmap.png", dpi=100, bbox_inches='tight')
        plt.close()
        
        return {
            "top_correlations": label_corrs.head(10).to_dict(),
            "numeric_features_count": len(filtered_cols)
        }
    else:
        print("  ååˆ†ãªæ•°å€¤ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return {"message": "ååˆ†ãªæ•°å€¤ç‰¹å¾´é‡ãªã—"}


def run_eda(train_path, text_col, label_col, output_dir):
    """EDAå®Ÿè¡Œã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ EDAåˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"ğŸ“ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: {train_path}")
    print(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆåˆ—: {text_col}")
    print(f"ğŸ·ï¸  ãƒ©ãƒ™ãƒ«åˆ—: {label_col}")
    print(f"ğŸ’¾ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("ğŸ“– ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    df = load_data_with_encoding(train_path)
    
    # åˆ—ã®è‡ªå‹•æ¤œå‡ºï¼ˆå¼•æ•°ã§æŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
    if not text_col:
        text_col = detect_text_column(df)
        print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’è‡ªå‹•æ¤œå‡º: {text_col}")
    
    if not label_col:
        label_col = detect_label_column(df)
        print(f"ğŸ·ï¸  ãƒ©ãƒ™ãƒ«åˆ—ã‚’è‡ªå‹•æ¤œå‡º: {label_col}")
    
    # åŸºæœ¬æƒ…å ±
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
    
    # æ¬ æå€¤ã®ç¢ºèª
    missing_info = df.isnull().sum()
    missing_cols = missing_info[missing_info > 0]
    if len(missing_cols) > 0:
        print(f"âš ï¸  æ¬ æå€¤ã‚ã‚Š: {len(missing_cols)}åˆ—")
    
    # ã‚µãƒãƒªãƒ‡ãƒ¼ã‚¿åˆæœŸåŒ–
    summary_data = {
        "dataset_shape": list(df.shape),
        "columns": list(df.columns),
        "null_counts": missing_info.to_dict(),
        "detected_text_col": text_col,
        "detected_label_col": label_col
    }
    
    # å„åˆ†æã‚’å®Ÿè¡Œ
    try:
        if text_col and text_col in df.columns:
            summary_data["text_length_stats"] = analyze_text_length(df, text_col, output_dir)
            summary_data["duplicates"] = analyze_duplicates(df, text_col, output_dir)
        else:
            print("âš ï¸  ãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    except Exception as e:
        print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        if label_col and label_col in df.columns:
            summary_data["label_stats"] = analyze_label_distribution(df, label_col, output_dir)
            summary_data["correlations"] = analyze_correlations(df, label_col, output_dir)
        else:
            print("âš ï¸  ãƒ©ãƒ™ãƒ«åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ©ãƒ™ãƒ«åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    except Exception as e:
        print(f"âŒ ãƒ©ãƒ™ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        summary_data["groups"] = analyze_groups(df, output_dir)
    except Exception as e:
        print(f"âŒ ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        summary_data["time_analysis"] = analyze_time_leakage(df, output_dir)
    except Exception as e:
        print(f"âŒ æ™‚ç³»åˆ—åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    # æœ€çµ‚ã‚µãƒãƒªãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    print("ğŸ’¾ ã‚µãƒãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ä¸­...")
    with open(f"{output_dir}/summary.json", "w", encoding="utf-8") as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®ä¸€è¦§
    artifacts = glob.glob(f"{output_dir}/*")
    artifacts = [os.path.basename(path) for path in artifacts]
    
    print()
    print("âœ… EDAåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"ğŸ“‚ ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ ({len(artifacts)}å€‹):")
    for artifact in sorted(artifacts):
        print(f"   - {artifact}")
    print(f"ğŸ’¾ ä¿å­˜å…ˆ: {os.path.abspath(output_dir)}")


def main():
    """CLIãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰CLIãƒ„ãƒ¼ãƒ«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python tools/eda_runner.py
  python tools/eda_runner.py --train-path data/train.csv --text-col comment_text --label-col target
  EDA_TRAIN_PATH=data/train.csv EDA_TEXT_COL=comment_text EDA_LABEL_COL=target python tools/eda_runner.py --out-dir output/eda
        """
    )
    
    parser.add_argument(
        '--train-path',
        type=str,
        help='è¨“ç·´ãƒ‡ãƒ¼ã‚¿CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæœªæŒ‡å®šæ™‚ã¯è‡ªå‹•æ¤œå‡ºï¼‰'
    )
    parser.add_argument(
        '--text-col',
        type=str,
        help='ãƒ†ã‚­ã‚¹ãƒˆåˆ—åï¼ˆæœªæŒ‡å®šæ™‚ã¯è‡ªå‹•æ¤œå‡ºï¼‰'
    )
    parser.add_argument(
        '--label-col',
        type=str,
        help='ãƒ©ãƒ™ãƒ«åˆ—åï¼ˆæœªæŒ‡å®šæ™‚ã¯è‡ªå‹•æ¤œå‡ºï¼‰'
    )
    parser.add_argument(
        '--out-dir',
        type=str,
        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæœªæŒ‡å®šæ™‚ã¯ç’°å¢ƒã«å¿œã˜ã¦è‡ªå‹•è¨­å®šï¼‰'
    )
    
    args = parser.parse_args()
    
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã®è¨­å®šå–å¾—
    train_path = args.train_path or find_train_csv()
    text_col = args.text_col or os.getenv('EDA_TEXT_COL')
    label_col = args.label_col or os.getenv('EDA_LABEL_COL')
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    if args.out_dir:
        out_dir = args.out_dir
    elif os.getenv('EDA_OUT_DIR'):
        out_dir = os.getenv('EDA_OUT_DIR')
    elif Path('/kaggle').exists():
        out_dir = os.getenv('WORKING_DIR', '/kaggle/working') + '/eda'
    else:
        out_dir = 'experiments/EXP001G/artifacts/eda'
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(out_dir, exist_ok=True)
    
    if not train_path:
        print("âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        print("   --train-path ã§æŒ‡å®šã™ã‚‹ã‹ã€EDA_TRAIN_PATHç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        print("   ã¾ãŸã¯ã€ä»¥ä¸‹ã®ãƒ‘ã‚¹ã®ã„ãšã‚Œã‹ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„:")
        print("   - data/train.csv")
        print("   - data/raw/train.csv")
        print("   - input/train.csv")
        print("   - input/*/train*.csv")
        print("   - dataset/train.csv")
        print("   - sample_train.csv")
        print("   - /kaggle/input/**/train*.csv (Kaggleç’°å¢ƒ)")
        return 1
    
    if not os.path.exists(train_path):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {train_path}")
        return 1
    
    try:
        run_eda(train_path, text_col, label_col, out_dir)
        return 0
    except Exception as e:
        print(f"âŒ EDAå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())