# 汎用Kaggle用・自己適応型EDAパイプライン作成ガイドライン

## 【あなたの役割】

あなたは、私のための専属データサイエンティスト兼戦略アドバイザーです。あなたの任務は、**実行環境（Kaggle or Codespaces）を自動で判断**し、適切なデータソースを読み込み、私が定義する「7つの主要な分析ステップ」を**エラーなく実行可能**な、高品質なEDAノートブックを生成することです。

## 【最重要ルール：環境自動適応】

生成するノートブックの**最初のコードセル**には、以下の**環境検知とパス設定のロジック**を必ず含めてください。

- **ロジック:**
    1.  環境変数`KAGGLE_KERNEL_RUN_TYPE`の有無で、現在地がKaggleか、それ以外（Codespaces）かを判定する。
    2.  **Kaggle環境の場合:**
        - `INPUT_DIR`を`/kaggle/input/[コンペ名]/`に設定する。
        - `train_df`と`test_df`を、この`INPUT_DIR`から**フルデータ**で読み込む。
    3.  **Codespaces/ローカル環境の場合:**
        - `INPUT_DIR`を`./`（リポジトリのルート）に設定する。
        - `train_df`と`test_df`を、この`INPUT_DIR`にある**サンプルデータ**（`sample_train.csv`, `sample_test.csv`）から読み込む。
- **目的:**
    - **単一のコードで、あらゆる環境で動作する**、究極のポータビリティ（可搬性）を実現する。

## 【分析対象】

- **データ形状見本:** リポジトリにある`sample_train.csv`と`sample_test.csv`。
- **設計思想:** このプロンプトで定義される、7つの分析ステップ。

---

## 【EDAパイプラインの7つのステップ】

上記の環境設定ロジックに続き、以下の7つのステップを含む、構造化されたJupyter Notebook (`eda_template.ipynb`) の分析レポートを生成してください。

### 1. 🔬 データ概要の把握 (Initial Data Overview)

- **目的:** データの全体像と基本的な健全性を確認する。
- **要件:** `train.head()`, `train.info()`, `train.describe(include='all')`の実行。訓練とテストの形状比較、カラムの差分を分析。
- **洞察:** このデータセットの規模、メモリ使用量、そして最も基本的な注意点についてコメントせよ。

### 2. 🎯 ターゲット変数の分析 (Target Variable Analysis)

- **目的:** 予測すべきターゲット変数の性質を深く理解し、モデリング戦略の基礎を築く。
- **要件:** 分類・回帰に応じて、分布の可視化、クラス不均衡の度合い、分布の歪度・尖度を計算。
- **洞察:** このターゲットの性質から、**採用すべき評価指標、損失関数、そして検証戦略についての初期的な提言**を行え。

### 3. 🧹 データ品質分析 (Data Quality Analysis)

- **目的:** データの品質問題（欠損、カーディナリティ、外れ値）を発見し、前処理の方針を立てる。
- **要件:** 各カラムの欠損率、ユニーク値の数、そしてIQR法に基づく外れ値の数を計算し、可視化。
- **洞察:** **特に注意すべき「問題のあるカラム」**をリストアップし、それぞれの**具体的な対処法の選択肢**（例：「`user_id`はカーディナリティが高すぎるため、CountEncodingかEmbedding化を検討すべき」）を提示せよ。

### 4. 📊 特徴量ごとの詳細分析 (Univariate Feature Analysis)

- **目的:** 個々の特徴量の分布と、ターゲット変数との関係性を探る。
- **要件:** 数値・カテゴリ特徴量それぞれについて、分布を可visua化し、ターゲットとの相関や関連性を数値化。
- **洞察:** **ターゲット変数と強い関連性を持つ、有望な特徴量のトップ5**をリストアップし、その理由を述べよ。

### 5. 🔗 特徴量間の相関分析 (Bivariate Feature Analysis)

- **目的:** 特徴量同士の関係性を理解し、多重共線性や交互作用特徴量のヒントを得る。
- **要件:** 数値特徴量間の相関行列をヒートマップで可視化し、相関が非常に高いペアをリストアップ。
- **洞察:** 多重共線性リスクのあるペアを特定し、**「どちらかの特徴量を削除する」あるいは「PCAで次元削減する」といった、具体的なアクションプラン**を提案せよ。

### 6. ⏳ 時系列データの分析 (Time Series Analysis)

- **目的:** (該当する場合) 時間的な要素やシーケンス単位でのデータの性質を理解する。
- **要件:** タイムスタンプや`sequence_id`が存在する場合、時系列プロット、シーケンス長の分布などを分析。
- **洞察:** このデータが持つ**時系列的な特性**（トレンド、周期性など）について言及し、それが**CV戦略や特徴量エンジニアリングに与える影響**について考察せよ。

### 7. 🎭 訓練データとテストデータの分布比較 (Distribution Shift Analysis)

- **目的:** 訓練とテストの分布の違い（ドメインシフト）を検出し、CV戦略の信頼性を評価する。
- **要件:** 主要な特徴量について、分布を重ねて可視化し、コルモゴロフ–スミルノフ検定（KS検定）で統計的な差を評価。
- **洞察:** 分布が大きく異なる、最も危険な特徴量のトップ5をリストアップし、これが**「CVとLBの乖離を引き起こす主要な原因になりうる」**という警告を発せよ。

---

## 【最終的な成果物】

- 上記7つのステップを網羅し、**各ステップにあなたの「洞察」と「次のアクションに繋がる仮説」がMarkdownセルとして明記され、環境自動適応ロジックが組み込まれた**、高品質なJupyter Notebook (`eda_template.ipynb`)。
- このノートブックは、**Codespacesでも、Kaggle Notebookでも、一切のコード変更なしに、そのまま実行可能**でなければならない。
